{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Conversation Dataset Generator\n",
    "\n",
    "## Deskripsi\n",
    "Notebook ini menghasilkan dataset percakapan multi-turn untuk melatih AI sebagai interviewer karier di platform Diploy.\n",
    "\n",
    "## Setup Awal\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "pip install openai pandas openpyxl tqdm python-dotenv\n",
    "```\n",
    "\n",
    "### 2. Set API Key\n",
    "**Untuk keamanan, JANGAN hardcode API key!**\n",
    "\n",
    "#### Opsi A: Environment Variable (Recommended)\n",
    "```bash\n",
    "export OPENAI_API_KEY='your-api-key-here'\n",
    "```\n",
    "\n",
    "#### Opsi B: File .env\n",
    "Buat file `.env` di root folder:\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "Lalu load dengan:\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "### 3. Persiapkan Data\n",
    "Pastikan file Excel `datasetmultiturncreateV2.xlsx` memiliki kolom:\n",
    "- Jenjang Pendidikan\n",
    "- Jurusan\n",
    "- Pelatihan\n",
    "- Sertifikasi\n",
    "- Pekerjaan Saat Ini\n",
    "- Pengalaman Kerja\n",
    "- Keterampilan\n",
    "- Area_Fungsi\n",
    "- Level\n",
    "\n",
    "## Output Format\n",
    "File JSONL dengan format:\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"...\"},\n",
    "    {\"role\": \"user\", \"content\": \"...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Mode Percakapan\n",
    "- **FAST_DIRECT**: Langsung rekomendasi\n",
    "- **FAST_SHORT**: 1 pertanyaan → rekomendasi\n",
    "- **MEDIUM**: 4-6 turn dengan 1-2 pertanyaan\n",
    "- **LONG**: 8-10 turn dengan 3-5 pertanyaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AauggwdgsE_2",
    "outputId": "3a57bcdc-3c79-47f2-ccf3-091f3650e1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /Users/irz/Downloads/dtp-data-pipeline-main/Pipeline Multiturn/Flagged_500_Per_Class/Rows_1-100\n",
      "Directory exists: True\n",
      "Found 45 Excel files\n",
      "   First few: ['Pengembangan_Produk_Digital_5.xlsx', 'Tata_Kelola_Teknologi_Informasi_9.xlsx', 'Teknologi_dan_Infrastruktur_6.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# PATH CONFIG\n",
    "# =========================\n",
    "# Untuk Google Colab, uncomment baris berikut:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATASET_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "\n",
    "DATASET_DIR = Path.cwd().parent / \"Flagged_500_Per_Class/Rows_1-100\"\n",
    "DATASET_DIR = str(DATASET_DIR)\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Directory exists: {os.path.exists(DATASET_DIR)}\")\n",
    "\n",
    "# List files untuk verifikasi\n",
    "if os.path.exists(DATASET_DIR):\n",
    "    files = os.listdir(DATASET_DIR)\n",
    "    excel_files = [f for f in files if f.endswith('.xlsx')]\n",
    "    print(f\"Found {len(excel_files)} Excel files\")\n",
    "    if excel_files:\n",
    "        print(f\"   First few: {excel_files[:3]}\")\n",
    "else:\n",
    "    print(\"WARNING: Directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup: Set API Key\n",
    "\n",
    "**Pilih salah satu cara berikut:**\n",
    "\n",
    "### Cara 1: Langsung di Notebook (Paling Cepat)\n",
    "Jalankan cell ini terlebih dahulu, lalu jalankan cell berikutnya:\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-your-actual-key-here'  # Ganti dengan key Anda\n",
    "```\n",
    "\n",
    "### Cara 2: Buat File .env\n",
    "Buat file `.env` di folder `/home/wildanaziz/dtp-data-pipeline/Pipeline Multiturn/script/` dengan isi:\n",
    "```\n",
    "OPENAI_API_KEY=sk-proj-your-actual-key-here\n",
    "```\n",
    "\n",
    "### Cara 3: Terminal (Persistent)\n",
    "```bash\n",
    "export OPENAI_API_KEY='sk-proj-your-actual-key-here'\n",
    "```\n",
    "Lalu restart kernel notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ PENTING: Restart Kernel\n",
    "\n",
    "**Jika Anda mendapat error ValueError pada format string:**\n",
    "\n",
    "1. **Restart Kernel**: Klik menu `Kernel` → `Restart Kernel` atau tekan tombol restart di toolbar\n",
    "2. **Jalankan ulang cell 2** (PATH CONFIG)\n",
    "3. **Jalankan ulang cell 6** (Main processing code)\n",
    "\n",
    "**Alasan**: File sudah diperbaiki, tapi kernel masih menyimpan kode lama di memori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pylUC3KETfJf",
    "outputId": "0e4e6750-68d2-493e-f13b-9c21233b8584"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irz/projects/Dataset SFT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-dotenv tidak terinstall, gunakan environment variable manual\n",
      "API Key loaded from .env file manually\n",
      "API Key: sk-or-v1-0db430b734e... (truncated for security)\n",
      "Output akan disimpan di: /Users/irz/Downloads/dtp-data-pipeline-main/Pipeline Multiturn/MultiturnDatasetOutput\n",
      "\n",
      "Memulai proses generation...\n",
      "Harap tunggu, ini akan memakan waktu...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Multi-File Dataset Generation\n",
      "============================================================\n",
      "Time: 2025-12-11 01:01:27\n",
      "Input Dir: /Users/irz/Downloads/dtp-data-pipeline-main/Pipeline Multiturn/Flagged_500_Per_Class/Rows_1-100\n",
      "Output Dir: /Users/irz/Downloads/dtp-data-pipeline-main/Pipeline Multiturn/MultiturnDatasetOutput\n",
      "Model: google/gemini-2.5-flash\n",
      "Batch size: 10\n",
      "Concurrent requests: 5\n",
      "============================================================\n",
      "\n",
      "Found 45 Excel files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/45 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: Pengembangan_Produk_Digital_5\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse failed (row 0, long, attempt 1): JSON parse error: Unterminated string starting at: line 56 column 16 (char 5857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   2%|▏         | 1/45 [00:20<14:43, 20.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Pengembangan_Produk_Digital_5 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Tata_Kelola_Teknologi_Informasi_9\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   4%|▍         | 2/45 [00:25<08:24, 11.73s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Tata_Kelola_Teknologi_Informasi_9 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Teknologi_dan_Infrastruktur_6\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   7%|▋         | 3/45 [00:31<06:22,  9.10s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Teknologi_dan_Infrastruktur_6 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Pengembangan_Produk_Digital_9\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   9%|▉         | 4/45 [00:37<05:23,  7.89s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Pengembangan_Produk_Digital_9 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Keamanan_Informasi_dan_Siber_3\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:  11%|█         | 5/45 [00:45<05:10,  7.77s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Keamanan_Informasi_dan_Siber_3 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Tata_Kelola_Teknologi_Informasi_5\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:  13%|█▎        | 6/45 [00:52<04:47,  7.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Tata_Kelola_Teknologi_Informasi_5 - 1 batches created\n",
      "\n",
      "============================================================\n",
      "Processing: Layanan_Teknologi_Informasi_1\n",
      "============================================================\n",
      "Columns detected: ['Jenjang_Pendidikan', 'Jurusan', 'Judul_Tugas_Akhir', 'Bidang_Pelatihan', 'Nama_Pelatihan', 'Sertifikasi', 'Bidang_Sertifikasi', 'Posisi_Pekerjaan', 'Deskripsi_tugas_dan_tanggung_jawab', 'Lama_Bekerja', 'Keterampilan', 'Area_Fungsi', 'Level_Okupasi']\n",
      "Total rows: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:  13%|█▎        | 6/45 [00:54<05:53,  9.06s/file]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 682\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMemulai proses generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    680\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHarap tunggu, ini akan memakan waktu...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_all_files(input_directory, output_directory)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 669\u001b[39m, in \u001b[36mprocess_all_files\u001b[39m\u001b[34m(input_dir, output_base)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[38;5;28mlen\u001b[39m(excel_files), desc=\u001b[33m\"\u001b[39m\u001b[33mFiles\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_pbar:\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m excel_file \u001b[38;5;129;01min\u001b[39;00m excel_files:\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m process_single_file(excel_file, output_base, file_pbar)\n\u001b[32m    671\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    672\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mALL DONE!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 629\u001b[39m, in \u001b[36mprocess_single_file\u001b[39m\u001b[34m(input_path, output_dir, file_pbar)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m process_batch(batch, w, batch_count, pbar)\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError writing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 555\u001b[39m, in \u001b[36mprocess_batch\u001b[39m\u001b[34m(df, writer, batch_num, pbar)\u001b[39m\n\u001b[32m    553\u001b[39m results = []\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m coro \u001b[38;5;129;01min\u001b[39;00m asyncio.as_completed(tasks):\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     out = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m    556\u001b[39m     results.append(out)\n\u001b[32m    557\u001b[39m     pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/Dataset SFT/.venv/lib/python3.12/asyncio/tasks.py:627\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait_for_one\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     f = \u001b[38;5;28;01mawait\u001b[39;00m done.get()\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m         \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    630\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/Dataset SFT/.venv/lib/python3.12/asyncio/queues.py:158\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._getters.append(getter)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    160\u001b[39m     getter.cancel()  \u001b[38;5;66;03m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import asyncio\n",
    "import random\n",
    "from openai import AsyncOpenAI, APIError, RateLimitError\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# load env var\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Load dari .env file\n",
    "    print(\".env file loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv tidak terinstall, gunakan environment variable manual\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .env: {e}\")\n",
    "\n",
    "# config env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    env_file = Path.cwd() / \".env\"\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY'):\n",
    "                    OPENAI_API_KEY = line.split('=')[1].strip().strip('\"').strip(\"'\")\n",
    "                    print(\"API Key loaded from .env file manually\")\n",
    "                    break\n",
    "    \n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"OPENAI_API_KEY TIDAK DITEMUKAN!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nCara Set API Key:\")\n",
    "        print(\"\\nOpsi 1: Buat file .env di folder ini:\")\n",
    "        print(f\"   Location: {Path.cwd()}\")\n",
    "        print(\"   Isi file .env:\")\n",
    "        print('   OPENAI_API_KEY=sk-proj-your-actual-key-here')\n",
    "        print(\"\\nOpsi 2: Set di notebook (temporary):\")\n",
    "        print(\"   Tambahkan cell baru dengan:\")\n",
    "        print(\"   import os\")\n",
    "        print(\"   os.environ['OPENAI_API_KEY'] = 'sk-proj-your-actual-key-here'\")\n",
    "        print(\"\\nOpsi 3: Set di terminal (persistent):\")\n",
    "        print(\"   export OPENAI_API_KEY='sk-proj-your-actual-key-here'\")\n",
    "        print(\"   Lalu restart kernel notebook\")\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        raise ValueError(\"OPENAI_API_KEY tidak ditemukan! Ikuti instruksi di atas.\")\n",
    "\n",
    "print(f\"API Key: {OPENAI_API_KEY[:20]}... (truncated for security)\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# atur batch size konsisten\n",
    "BATCH_SIZE = 25\n",
    "MAX_TOKENS = 1400\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "RETRY_LIMIT = 3\n",
    "RETRY_DELAY = 3  \n",
    "CONCURRENT_REQUESTS = 1\n",
    "\n",
    "# pilih model list yang ada di openrouter pastiin pake yg gpt aje\n",
    "MODEL_NAME = \"google/gemini-2.5-flash\"  \n",
    "\n",
    "# output dir config\n",
    "OUTPUT_BASE_DIR = Path.cwd().parent / \"MultiturnDatasetOutput_1-100\"  \n",
    "\n",
    "print(f\"Output akan disimpan di: {OUTPUT_BASE_DIR}\")\n",
    "\n",
    "# SYSTEM PROMPT (LOCKED) — versi baru sesuai desain percakapan Diploy\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Anda adalah interviewer dari platform talenta digital Diploy khusus Area Fungsi. Tugas Anda adalah menggali detail kompetensi talenta berdasarkan data awal yang diberikan, meluruskan jawaban yang kurang relevan, dan memastikan informasi yang terkumpul cukup tajam untuk pemetaan Area Fungsi dan Level Okupasi. Gunakan bahasa Indonesia yang baik dan benar, tetap profesional, dan jangan menggunakan bahasa gaul atau singkatan informal.\"\n",
    ")\n",
    "\n",
    "# GLOBAL RULES – format percakapan & output JSON yang baru\n",
    "GLOBAL_RULES = \"\"\"\n",
    "ATURAN FORMAT OUTPUT\n",
    "====================\n",
    "1. Output WAJIB berupa ARRAY JSON VALID yang berisi daftar pesan percakapan.\n",
    "2. Setiap elemen array WAJIB memiliki:\n",
    "   - \"role\": hanya boleh \"user\" atau \"assistant\"\n",
    "   - \"content\": string isi pesan.\n",
    "3. TIDAK BOLEH ada objek lain di luar array tersebut dan TIDAK BOLEH ada teks di luar JSON.\n",
    "\n",
    "STRUKTUR WAJIB\n",
    "==============\n",
    "1) Pesan pertama:\n",
    "   - role = \"user\"\n",
    "   - pesan pertama selalu dari user. content berisi ringkasan profil dari data yang diberikan dengan format:\n",
    "     \"Berikut data singkat saya:\\\\n<Field 1>: <isi>\\\\n<Field 2>: <isi>\\\\n...\"\n",
    "   - Informasi yang digunakan HANYA boleh diambil dari bagian PROFILE DATA (tidak boleh diubah).\n",
    "\n",
    "2) Pesan-pesan berikutnya:\n",
    "   - Bergantian antara \"assistant\" dan \"user\" sesuai kebutuhan percakapan.\n",
    "   - Setelah pesan pertama dari user, assistant mengawali pembicaraan dengan contoh seperti ini: \"Halo, terima kasih sudah menyempatkan waktu untuk mengikuti interview ini. Saya akan mengajukan beberapa pertanyaan terkait kompetensi Anda. (diteruskan dengan pertanyaan pertama)\"\n",
    "   - Assistant:\n",
    "     * hanya boleh menanyakan hal yang relevan dengan pendidikan, tugas akhir, pelatihan, sertifikasi, pengalaman kerja, dan keterampilan yang ada di data.\n",
    "     * WAJIB menggali lebih dalam jika jawaban user terlalu singkat atau masih umum.\n",
    "     * WAJIB meluruskan jika jawaban user keluar konteks, lalu mengembalikan fokus ke pertanyaan.\n",
    "\n",
    "3) Pesan terakhir:\n",
    "   - role = \"assistant\"\n",
    "   - content WAJIB diawali dengan tag persis: \"[END OF CHAT]\".\n",
    "   - Setelah tag tersebut:\n",
    "     * ucapkan terima kasih karena sudah melakukan interview,\n",
    "     * sebutkan secara eksplisit Area Fungsi dan Level yang sudah dipetakan (lihat TEMPLATE RINGKASAN AKHIR),\n",
    "     * tambahkan 1–3 kalimat alasan singkat dan to the point, berdasarkan:\n",
    "       - latar belakang pendidikan,\n",
    "       - pengalaman kerja,\n",
    "       - pelatihan/sertifikasi,\n",
    "       - dan keterampilan yang muncul di data dan jawaban user.\n",
    "   - TIDAK BOLEH ada pertanyaan baru setelah bagian ringkasan akhir.\n",
    "\n",
    "GAYA BAHASA\n",
    "===========\n",
    "- Assistant:\n",
    "  * menggunakan bahasa Indonesia yang baku, jelas, dan profesional,\n",
    "  * menggunakan kata ganti \"Anda\",\n",
    "  * TIDAK menggunakan singkatan informal seperti \"yg\", \"gk\", \"ga\", \"dr\", \"tp\", \"utk\", \"sdh\", \"blm\", dsb.\n",
    "- User:\n",
    "  * secara umum boleh menjawab dengan bahasa yang natural,\n",
    "  * sesekali boleh muncul singkatan ringan, tetapi tetap sopan dan mudah dipahami.\n",
    "\n",
    "HAL YANG DILARANG\n",
    "=================\n",
    "- Menginvensi sertifikasi, pelatihan, pekerjaan, atau keterampilan baru yang tidak ada di data.\n",
    "- Menulis JSON di dalam string \"content\".\n",
    "- Mengubah target Area Fungsi dan Level yang sudah diberikan di prompt.\n",
    "\"\"\"\n",
    "\n",
    "# MODE RULES - Deskripsi eksplisit dalam bahasa Indonesia\n",
    "MODE_RULES = {\n",
    "    \"fast_direct\": \"PERCAKAPAN LANGSUNG TANPA PERTANYAAN: Tidak ada pertanyaan interview. User memberikan data profil, lalu assistant langsung memberikan kesimpulan rekomendasi Area Fungsi dan Level beserta alasannya. Total 2 pesan (1 user, 1 assistant dengan [END OF CHAT]).\",\n",
    "    \n",
    "    \"fast_short\": \"PERCAKAPAN SINGKAT: Assistant mengajukan 1-2 pertanyaan singkat untuk konfirmasi atau klarifikasi data penting. Total 2-5 pesan bergantian, diakhiri assistant dengan [END OF CHAT].\",\n",
    "    \n",
    "    \"medium\": \"PERCAKAPAN SEDANG: Assistant melakukan interview dengan 3-5 pertanyaan untuk menggali kompetensi lebih dalam terkait pengalaman kerja, pelatihan, dan keterampilan. Total 6-11 pesan bergantian, diakhiri assistant dengan [END OF CHAT].\",\n",
    "    \n",
    "    \"long\": \"PERCAKAPAN PANJANG: Assistant melakukan interview mendalam dengan 6-8 pertanyaan detail mengenai pendidikan, tugas akhir, pelatihan, sertifikasi, pengalaman kerja, tanggung jawab, dan keterampilan. Jika user menjawab kurang detail, assistant menggali lebih dalam. Total 12-15 pesan bergantian, diakhiri assistant dengan [END OF CHAT].\"\n",
    "}\n",
    "\n",
    "FAST_VARIANTS = [\"fast_direct\", \"fast_short\"]\n",
    "\n",
    "# Normalisasi\n",
    "MISSING = {\"\", \"-\", \"–\", \"—\", \"none\", \"nan\", \"n/a\", \"null\", \"tidak ada\"}\n",
    "\n",
    "def normalize(v):\n",
    "    \"\"\"Normalisasi nilai kosong/missing.\"\"\"\n",
    "    if v is None or pd.isna(v):\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    return None if s.lower() in MISSING else s\n",
    "\n",
    "# Extract fields\n",
    "def extract_fields(row):\n",
    "    \"\"\"Ekstrak field public (untuk conversation) dan hidden (untuk validasi).\"\"\"\n",
    "    pub_map = {\n",
    "        \"Jenjang_Pendidikan\": \"Jenjang Pendidikan\",  \n",
    "        \"Jurusan\": \"Jurusan\",\n",
    "        \"Judul_Tugas_Akhir\": \"Judul Tugas Akhir\",\n",
    "        \"Bidang_Pelatihan\": \"Bidang Pelatihan\",  \n",
    "        \"Nama_Pelatihan\": \"Nama Pelatihan\",  \n",
    "        \"Sertifikasi\": \"Sertifikasi\",\n",
    "        \"Bidang_Sertifikasi\": \"Bidang Sertifikasi\",\n",
    "        \"Posisi_Pekerjaan\": \"Posisi Pekerjaan\",  \n",
    "        \"Deskripsi_tugas_dan_tanggung_jawab\": \"Deskripsi Tugas dan Tanggung Jawab\",  \n",
    "        \"Lama_Bekerja\": \"Lama Bekerja\",  \n",
    "        \"Keterampilan\": \"Keterampilan\",\n",
    "    }\n",
    "\n",
    "    hid_map = {\n",
    "        \"Area_Fungsi\": \"Area Fungsi\", \n",
    "        \"Level_Okupasi\": \"Level\"\n",
    "    }\n",
    "\n",
    "    public, hidden = {}, {}\n",
    "\n",
    "    for c, k in pub_map.items():\n",
    "        v = normalize(row.get(c))\n",
    "        if v:\n",
    "            public[k] = v\n",
    "\n",
    "    for c, k in hid_map.items():\n",
    "        v = normalize(row.get(c))\n",
    "        if v:\n",
    "            if k == \"Level\":\n",
    "                try:\n",
    "                    v = str(int(float(v)))\n",
    "                except Exception:\n",
    "                    v = v\n",
    "            hidden[k] = v\n",
    "\n",
    "    return public, hidden\n",
    "\n",
    "# Build user intro\n",
    "def make_user_intro(public):\n",
    "    \"\"\"Generate user introduction dari data profil - mencakup SEMUA kolom.\"\"\"\n",
    "    if not public:\n",
    "        return \"Halo, saya ingin mengikuti asesmen karier.\"\n",
    "    \n",
    "    # Format: \"Berikut data singkat saya:\\n<Field>: <Value>\\n...\"\n",
    "    lines = [\"Halo, saya ingin mengikuti asesmen karier. Berikut data singkat saya:\"]\n",
    "    \n",
    "    for field, value in public.items():\n",
    "        lines.append(f\"{field}: {value}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Build prompt\n",
    "def build_prompt(row, mode_key):\n",
    "    \"\"\"Generate prompt untuk GPT model sesuai format baru Diploy.\"\"\"\n",
    "    public, hidden = extract_fields(row)\n",
    "\n",
    "    # PROFILE DATA untuk referensi model\n",
    "    pdesc = \"\\n\".join([f\"- {k}: {v}\" for k, v in public.items()])\n",
    "\n",
    "    # Ambil level & area fungsi dari hidden\n",
    "    level_val = hidden.get(\"Level\", \"-\")\n",
    "    try:\n",
    "        level_val = str(int(float(level_val)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    area_val = hidden.get(\"Area Fungsi\", \"\")\n",
    "\n",
    "    return f\"\"\"\n",
    "PROFILE DATA (REFERENSI, JANGAN DIUBAH APA ADANYA):\n",
    "{pdesc}\n",
    "\n",
    "KONTEKS:\n",
    "- Anda akan mensimulasikan percakapan antara user dan interviewer di platform digital talent pool\n",
    "- Area Fungsi dan Level akan dipetakan berdasarkan data kualifikasi talenta dan hasil jawaban interview. namun dari excel ini sudah ada Area Fungsi dan Level nya untuk anda mengetahui konteks simulasi pembicaraan yang akan dibuat. Data Area Fungsi dan Level TIDAK BOLEH diubah:\n",
    "  * Area Fungsi: {area_val}\n",
    "  * Level Okupasi: {level_val}\n",
    "\n",
    "MODE PERCAPAKAN:\n",
    "{MODE_RULES[mode_key]}\n",
    "\n",
    "ATURAN FORMAT & PERILAKU:\n",
    "{GLOBAL_RULES}\n",
    "\n",
    "INSTRUKSI KHUSUS:\n",
    "- Pesan pertama (role=\"user\") HARUS berupa ringkasan profil dengan pola:\n",
    "  \"Berikut data singkat saya:\\\\n<Field 1>: <isi>\\\\n<Field 2>: <isi>\\\\n...\"\n",
    "  dan ISINYA wajib konsisten dengan PROFILE DATA di atas. Tidak boleh mengubah data\n",
    "- Anda bebas mengatur panjang percakapan dan jumlah turn (pertanyaan interview dan balasan) sesuai MODE, tetapi WAJIB menjaga fokus pada data yang dimiliki user dan menggali informasi di user hingga dirasa informasi yang didapat sudah cukup untuk memetakan user ke Area Fungsi dan Level tertentu.\n",
    "\n",
    "TEMPLATE RINGKASAN AKHIR (WAJIB DIPAKAI DI TURN TERAKHIR ASSISTANT, SILAKAN SESUAIKAN BAGIAN ALASAN):\n",
    "\"[END OF CHAT] Terima kasih telah melakukan interview. Setelah saya pertimbangkan dari data yang Anda miliki dan interview yang sudah kita lakukan, Anda mendapat Area Fungsi {area_val} dan Level {level_val}. Anda memiliki kompetensi ... (penjelasan singkat alasan rea) <RESULT>{{\\\"area_fungsi\\\":\\\"{area_val}\\\", \\\"level\\\":{level_val}}}</RESULT>\"\n",
    "\n",
    "TUGAS ANDA:\n",
    "- Hasilkan SATU ARRAY JSON VALID yang berisi daftar pesan percakapan.\n",
    "- Setiap elemen array mempunyai \"role\" dan \"content\".\n",
    "- System prompt TIDAK BOLEH dimasukkan ke dalam array (system prompt sudah diberikan terpisah).\n",
    "\"\"\"\n",
    "\n",
    "# enhanced validation function\n",
    "def validate_conversation_structure(messages):\n",
    "    \"\"\"\n",
    "    Validasi struktur conversation untuk memastikan format yang benar\n",
    "    sesuai desain SFT Diploy (tanpa system di dalam array).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not isinstance(messages, list):\n",
    "        return False, \"Messages bukan list\"\n",
    "    \n",
    "    if len(messages) < 2:\n",
    "        return False, f\"Conversation terlalu pendek: {len(messages)} messages\"\n",
    "\n",
    "    # Pesan pertama harus user\n",
    "    first = messages[0]\n",
    "    if first.get(\"role\") != \"user\":\n",
    "        return False, f\"Pesan pertama bukan 'user' tetapi {first.get('role')}\"\n",
    "\n",
    "    # Pesan terakhir harus assistant\n",
    "    last = messages[-1]\n",
    "    if last.get(\"role\") != \"assistant\":\n",
    "        return False, f\"Pesan terakhir bukan 'assistant' tetapi {last.get('role')}\"\n",
    "\n",
    "    # Pesan terakhir harus diawali [END OF CHAT]\n",
    "    last_content = last.get(\"content\", \"\")\n",
    "    if not isinstance(last_content, str) or not last_content.strip().startswith(\"[END OF CHAT]\"):\n",
    "        return False, \"Pesan terakhir assistant wajib diawali '[END OF CHAT]' dan diakhiri <RESULT>{\\\"area_fungsi\\\":\\\"{area_val}\\\", \\\"level\\\":{level_val}}</RESULT>\"\n",
    "\n",
    "    # Cek korupsi JSON di dalam string\n",
    "    for i, msg in enumerate(messages):\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        if isinstance(content, str):\n",
    "            stripped = content.strip()\n",
    "            if stripped.startswith(\"[{\") or stripped.startswith(\"[\\n  {\"):\n",
    "                return False, f\"Message {i} ({msg.get('role')}) contains JSON array as string (corruption detected)\"\n",
    "            if '\\\\\"role\\\\\"' in content or '\\\\\\\"role\\\\\\\"' in content:\n",
    "                return False, f\"Message {i} ({msg.get('role')}) contains escaped JSON (corruption detected)\"\n",
    "\n",
    "    # Cek field wajib & role valid\n",
    "    for i, msg in enumerate(messages):\n",
    "        if \"role\" not in msg:\n",
    "            return False, f\"Message {i} missing 'role' field\"\n",
    "        if \"content\" not in msg:\n",
    "            return False, f\"Message {i} missing 'content' field\"\n",
    "        if msg[\"role\"] not in (\"user\", \"assistant\"):\n",
    "            return False, f\"Message {i} has invalid role: {msg['role']}\"\n",
    "\n",
    "    return True, \"Valid\"\n",
    "\n",
    "def clean_and_parse_json(raw_text):\n",
    "    \"\"\"\n",
    "    Advanced JSON cleaning and parsing dengan multiple strategies.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (parsed_array, success, error_message)\n",
    "    \"\"\"\n",
    "    # Stage 1: Basic cleaning\n",
    "    cleaned = (\n",
    "        raw_text.replace(\"```json\", \"\")\n",
    "                .replace(\"```\", \"\")\n",
    "                .replace(\"\\n\\n\", \"\\n\")\n",
    "                .strip()\n",
    "    )\n",
    "    \n",
    "    # Stage 2: Remove common prefixes\n",
    "    prefixes = [\n",
    "        \"Here is the JSON array:\",\n",
    "        \"Berikut adalah array JSON:\",\n",
    "        \"JSON output:\",\n",
    "        \"Output JSON:\",\n",
    "        \"Here's the conversation:\",\n",
    "        \"Berikut percakapannya:\",\n",
    "    ]\n",
    "    for prefix in prefixes:\n",
    "        if cleaned.startswith(prefix):\n",
    "            cleaned = cleaned[len(prefix):].strip()\n",
    "    \n",
    "    # Stage 3: Direct parse attempt\n",
    "    if cleaned.startswith(\"[\") and cleaned.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(cleaned)\n",
    "            return arr, True, None\n",
    "        except json.JSONDecodeError as e:\n",
    "            # Try repair\n",
    "            try:\n",
    "                repaired = re.sub(r',\\s*}', '}', cleaned)\n",
    "                repaired = re.sub(r',\\s*]', ']', repaired)\n",
    "                arr = json.loads(repaired)\n",
    "                return arr, True, None\n",
    "            except:\n",
    "                pass \n",
    "    \n",
    "    # Stage 4: Extract array substring\n",
    "    if \"[\" in cleaned and \"]\" in cleaned:\n",
    "        try:\n",
    "            start = cleaned.index(\"[\")\n",
    "            end = cleaned.rindex(\"]\") + 1\n",
    "            extracted = cleaned[start:end]\n",
    "            \n",
    "            arr = json.loads(extracted)\n",
    "            return arr, True, None\n",
    "        except json.JSONDecodeError as e:\n",
    "            # Try repair on extracted\n",
    "            try:\n",
    "                repaired = re.sub(r',\\s*}', '}', extracted)\n",
    "                repaired = re.sub(r',\\s*]', ']', repaired)\n",
    "                arr = json.loads(repaired)\n",
    "                return arr, True, None\n",
    "            except:\n",
    "                return None, False, f\"JSON parse error: {str(e)}\"\n",
    "    \n",
    "    return None, False, \"No valid JSON array found in response\"\n",
    "\n",
    "# OpenAI call with ENHANCED error handling and validation\n",
    "async def call_api(prompt, row_index=None, mode=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Call OpenAI API dengan retry mechanism, validation, dan error handling.\n",
    "    Versi ini:\n",
    "    - Mengharapkan output berupa ARRAY JSON pesan tanpa system.\n",
    "    - Hanya mengizinkan role 'user' dan 'assistant' di dalam array.\n",
    "    - Memastikan pesan terakhir diawali '[END OF CHAT]'.\n",
    "    \"\"\"\n",
    "    for attempt in range(RETRY_LIMIT):\n",
    "        try:\n",
    "            # Adjust temperature based on attempt (lower = more deterministic)\n",
    "            attempt_temp = max(0.3, TEMPERATURE - (attempt * 0.1))\n",
    "            \n",
    "            resp = await client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                temperature=attempt_temp,\n",
    "            )\n",
    "\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "            # Parse with enhanced cleaning\n",
    "            parsed_arr, parse_success, parse_error = clean_and_parse_json(raw)\n",
    "            \n",
    "            if not parse_success:\n",
    "                print(f\"Parse failed (row {row_index}, {mode}, attempt {attempt+1}): {parse_error}\")\n",
    "                \n",
    "                # Retry dengan parameter berbeda\n",
    "                if attempt < RETRY_LIMIT - 1:\n",
    "                    await asyncio.sleep(RETRY_DELAY)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Fallback: kembalikan percakapan minimal dengan END OF CHAT\n",
    "                    return [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Berikut data singkat saya:\\n(terjadi kegagalan parsing output percakapan).\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": '[END OF CHAT] Terima kasih telah melakukan interview. Namun terjadi kesalahan teknis saat memproses percakapan. <RESULT>{\"area_fungsi\":\"unknown\", \"level\":null}</RESULT>'\n",
    "                        },\n",
    "                    ]\n",
    "            \n",
    "            # Bersihkan & filter hanya role user/assistant\n",
    "            cleaned_arr = []\n",
    "            for m in parsed_arr:\n",
    "                if not isinstance(m, dict):\n",
    "                    continue\n",
    "                role = m.get(\"role\")\n",
    "                content = m.get(\"content\")\n",
    "                if role not in (\"user\", \"assistant\"):\n",
    "                    # Buang system / role lain jika ada\n",
    "                    continue\n",
    "                cleaned_arr.append({\"role\": role, \"content\": content})\n",
    "            \n",
    "            # CRITICAL: Validate conversation structure\n",
    "            is_valid, validation_error = validate_conversation_structure(cleaned_arr)\n",
    "            \n",
    "            if not is_valid:\n",
    "                print(f\"Validation failed (row {row_index}, {mode}, attempt {attempt+1}): {validation_error}\")\n",
    "                \n",
    "                # Retry dengan parameter berbeda\n",
    "                if attempt < RETRY_LIMIT - 1:\n",
    "                    await asyncio.sleep(RETRY_DELAY)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Fallback: kembalikan percakapan minimal yang valid\n",
    "                    return [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Berikut data singkat saya:\\n(percakapan tidak valid menurut aturan).\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": '[END OF CHAT] Terima kasih telah melakukan interview. Namun percakapan yang dihasilkan tidak memenuhi aturan format. <RESULT>{\"area_fungsi\":\"unknown\", \"level\":null}</RESULT>'\n",
    "                        },\n",
    "                    ]\n",
    "            \n",
    "            # Sukses\n",
    "            return cleaned_arr\n",
    "\n",
    "        except (APIError, RateLimitError) as e:\n",
    "            print(f\"API error (row {row_index}, {mode}, attempt {attempt+1}/{RETRY_LIMIT}): {e}\")\n",
    "            await asyncio.sleep(RETRY_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error (row {row_index}, {mode}, attempt {attempt+1}): {e}\")\n",
    "            await asyncio.sleep(RETRY_DELAY)\n",
    "\n",
    "    # All retries exhausted\n",
    "    print(f\"Row {row_index} ({mode}) FAILED after {RETRY_LIMIT} attempts\")\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Berikut data singkat saya:\\n(semua percobaan pemanggilan API gagal).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '[END OF CHAT] Terima kasih telah melakukan interview. Namun semua percobaan pemanggilan API gagal. <RESULT>{\"area_fungsi\":\"unknown\", \"level\":null}</RESULT>'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Batch processing with semaphore\n",
    "SEM = asyncio.Semaphore(CONCURRENT_REQUESTS)\n",
    "\n",
    "async def safe_process_row(row, row_index):\n",
    "    \"\"\"Process single row dengan concurrency control dan validation - 1 row = 1 JSONL.\"\"\"\n",
    "    async with SEM:\n",
    "        # Ekstrak data untuk menentukan mode\n",
    "        judul_tugas_akhir = normalize(row.get(\"Judul_Tugas_Akhir\", \"\"))\n",
    "        sertifikasi = normalize(row.get(\"Sertifikasi\", \"\"))\n",
    "        posisi_pekerjaan = normalize(row.get(\"Posisi_Pekerjaan\", \"\"))\n",
    "        level_okupasi = normalize(row.get(\"Level_Okupasi\", \"\"))\n",
    "        jenjang_pendidikan = normalize(row.get(\"Jenjang_Pendidikan\", \"\"))\n",
    "        \n",
    "        # Cek kondisi untuk medium/long (data tidak lengkap)\n",
    "        is_incomplete = (\n",
    "            judul_tugas_akhir and \"tidak ada tugas akhir\" in judul_tugas_akhir.lower() or\n",
    "            sertifikasi and \"belum memiliki sertifikasi\" in sertifikasi.lower() or\n",
    "            posisi_pekerjaan and \"belum memiliki pengalaman kerja\" in posisi_pekerjaan.lower()\n",
    "        )\n",
    "        \n",
    "        # Cek kondisi khusus: Level 9 + S3 = fast_direct\n",
    "        is_level_9_s3 = False\n",
    "        if level_okupasi and jenjang_pendidikan:\n",
    "            try:\n",
    "                level_val = str(int(float(level_okupasi)))\n",
    "                is_level_9_s3 = (level_val == \"9\" and \"s3\" in jenjang_pendidikan.lower())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Tentukan mode berdasarkan kondisi\n",
    "        if is_level_9_s3:\n",
    "            mode = \"fast_direct\"\n",
    "        elif is_incomplete:\n",
    "            # Data tidak lengkap -> medium atau long\n",
    "            mode = random.choice([\"medium\", \"long\"])\n",
    "        else:\n",
    "            # Data lengkap -> medium atau fast_direct\n",
    "            mode = random.choice([\"medium\", \"fast_direct\"])\n",
    "        \n",
    "        prompt = build_prompt(row, mode)\n",
    "        \n",
    "        # Call API sekali saja\n",
    "        try:\n",
    "            result = await call_api(prompt, row_index, mode)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Exception (row {row_index}, {mode}): {str(e)}\")\n",
    "            return [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Berikut data singkat saya:\\n(terjadi error saat memproses).\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"[END OF CHAT] Terima kasih telah melakukan interview. Namun terjadi error: {str(e)} <RESULT>{{\\\"area_fungsi\\\":\\\"unknown\\\", \\\"level\\\":null}}</RESULT>\"\n",
    "                },\n",
    "            ]\n",
    "\n",
    "async def process_batch(df, writer, batch_num, pbar):\n",
    "    \"\"\"Process batch of rows dengan validation - 1 row = 1 JSONL.\"\"\"\n",
    "    tasks = []\n",
    "    for idx, row in df.iterrows():\n",
    "        tasks.append(safe_process_row(row, idx))\n",
    "    \n",
    "    results = []\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        out = await coro\n",
    "        results.append(out)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Write results dengan final validation\n",
    "    valid_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for msgs in results:\n",
    "        # Final check before writing\n",
    "        is_valid, _ = validate_conversation_structure(msgs)\n",
    "        \n",
    "        if is_valid:\n",
    "            # Format output dengan wrapper \"messages\" dan system di dalamnya\n",
    "            formatted_messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "            ] + msgs\n",
    "            \n",
    "            obj = {\n",
    "                \"messages\": formatted_messages\n",
    "            }\n",
    "            writer.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            # Skip corrupt conversations\n",
    "            error_count += 1\n",
    "            print(f\"Skipped corrupt conversation\")\n",
    "    \n",
    "    if error_count > 0:\n",
    "        print(f\"Batch {batch_num}: {valid_count} valid, {error_count} skipped\")\n",
    "\n",
    "# PROCESS SINGLE FILE\n",
    "async def process_single_file(input_path, output_dir, file_pbar=None):\n",
    "    \"\"\"Process satu file Excel.\"\"\"\n",
    "    file_name = Path(input_path).stem\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {file_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Read Excel with error handling\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "        print(f\"Columns detected: {df.columns.tolist()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File tidak ditemukan: {input_path}\")\n",
    "        if file_pbar:\n",
    "            file_pbar.update(1)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error membaca Excel: {e}\")\n",
    "        if file_pbar:\n",
    "            file_pbar.update(1)\n",
    "        return\n",
    "    \n",
    "    total = len(df)\n",
    "    print(f\"Total rows: {total}\")\n",
    "    \n",
    "    # Create output directory for this file\n",
    "    file_output_dir = output_dir / file_name\n",
    "    os.makedirs(file_output_dir, exist_ok=True)\n",
    "    \n",
    "    batch_count = 1\n",
    "    \n",
    "    # Progress bar for rows\n",
    "    with tqdm(total=total, desc=f\"  Rows ({file_name})\", unit=\"row\", leave=False) as pbar:\n",
    "        for i in range(0, total, BATCH_SIZE):\n",
    "            batch = df.iloc[i:i + BATCH_SIZE]\n",
    "            \n",
    "            file_path = f\"{file_output_dir}/batch_{batch_count:03d}.jsonl\"\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as w:\n",
    "                    await process_batch(batch, w, batch_count, pbar)\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing batch {batch_count}: {e}\")\n",
    "            \n",
    "            batch_count += 1\n",
    "    \n",
    "    print(f\"Done: {file_name} - {batch_count - 1} batches created\")\n",
    "    \n",
    "    if file_pbar:\n",
    "        file_pbar.update(1)\n",
    "\n",
    "# MAIN - PROCESS ALL FILES\n",
    "async def process_all_files(input_dir, output_base):\n",
    "    \"\"\"Process semua file Excel di direktori input.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Multi-File Dataset Generation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Input Dir: {input_dir}\")\n",
    "    print(f\"Output Dir: {output_base}\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Concurrent requests: {CONCURRENT_REQUESTS}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Find all Excel files\n",
    "    excel_files = list(Path(input_dir).glob(\"*.xlsx\"))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(\"Tidak ada file Excel ditemukan!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(excel_files)} Excel files\\n\")\n",
    "    \n",
    "    # Create base output directory\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    \n",
    "    # Process files sequentially (untuk menghindari rate limit)\n",
    "    with tqdm(total=len(excel_files), desc=\"Files\", unit=\"file\") as file_pbar:\n",
    "        for excel_file in excel_files:\n",
    "            await process_single_file(excel_file, output_base, file_pbar)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ALL DONE!\")\n",
    "    print(f\"Output: {output_base}\")\n",
    "    print(f\"Processed: {len(excel_files)} files\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "input_directory = DATASET_DIR\n",
    "output_directory = OUTPUT_BASE_DIR\n",
    "print(\"\\nMemulai proses generation...\")\n",
    "print(\"Harap tunggu, ini akan memakan waktu...\\n\")\n",
    "\n",
    "await process_all_files(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
